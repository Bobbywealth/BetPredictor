#!/usr/bin/env python3
"""
Quick test of the Confidence Calibration System
"""

import sys
sys.path.append('.')

from utils.confidence_calibration import ConfidenceCalibrator

def test_confidence_calibration():
    """Test the confidence calibration system"""
    
    print("ðŸ§ª Testing Confidence Calibration System...")
    print("=" * 50)
    
    calibrator = ConfidenceCalibrator()
    
    # Test scenarios
    test_cases = [
        {
            'name': 'High Confidence NBA Game',
            'confidence': 0.85,
            'context': {
                'sport': 'NBA',
                'model_type': 'NBA Quantitative',
                'data_quality_score': 0.8
            }
        },
        {
            'name': 'Medium Confidence MLB Game',
            'confidence': 0.68,
            'context': {
                'sport': 'MLB',
                'model_type': 'MLB Quantitative',
                'data_quality_score': 0.6
            }
        },
        {
            'name': 'Low Data Quality NFL Game',
            'confidence': 0.75,
            'context': {
                'sport': 'NFL',
                'model_type': 'Generic',
                'data_quality_score': 0.3
            }
        },
        {
            'name': 'College Football (High Variance)',
            'confidence': 0.80,
            'context': {
                'sport': 'NCAAF',
                'model_type': 'NCAAF Quantitative',
                'data_quality_score': 0.7
            }
        }
    ]
    
    for test_case in test_cases:
        print(f"\nðŸ“Š {test_case['name']}")
        print("-" * 30)
        
        result = calibrator.calibrate_confidence(
            test_case['confidence'],
            test_case['context']
        )
        
        print(f"Raw Confidence:        {result['raw_confidence']:.1%}")
        print(f"Governed Confidence:   {result['governed_confidence']:.1%}")
        print(f"Calibrated Confidence: {result['calibrated_confidence']:.1%}")
        print(f"Adjustment:            {result['calibration_adjustment']:+.1%}")
        print(f"Reliability Score:     {result['reliability_score']:.1%}")
        print(f"Recommendation Tier:   {result['recommendation_tier']}")
        print(f"Explanation:           {result['confidence_explanation']}")
        print(f"Calibration Quality:   {result['calibration_quality']}")
    
    # Test calibration report
    print("\nðŸ“ˆ Calibration Report")
    print("=" * 30)
    
    report = calibrator.get_calibration_report()
    print(f"Overall Calibration: {report['overall_calibration']}")
    
    for sport, data in report['sports_breakdown'].items():
        print(f"\n{sport}:")
        print(f"  Total Predictions: {data['total_predictions']}")
        print(f"  Accuracy: {data['accuracy']:.1%}")
        print(f"  Calibration Error: {data['calibration_error']:.3f}")
        print(f"  Quality: {data['calibration_quality']}")
    
    print("\nâœ… Confidence Calibration Test Complete!")
    print("\nðŸŽ¯ Key Features Verified:")
    print("  âœ… Sport-specific governance (caps)")
    print("  âœ… Data quality adjustments")
    print("  âœ… Historical calibration mapping")
    print("  âœ… Reliability scoring")
    print("  âœ… Recommendation tiers")
    print("  âœ… Conservative bias protection")

if __name__ == "__main__":
    test_confidence_calibration()
